{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cc4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffe763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\sapir\\out_of_onedrive\\dina\\CalibVEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae4a120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sapir\\anaconda3\\envs\\env_image_processing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import definitions as d\n",
    "import torch\n",
    "from data_classes import METHODS_TO_ESM, MutationVariantSet\n",
    "from mutation_record import MutationRecord\n",
    "from utils import is_disordered\n",
    "from load_cluster_lookup import get_cluster_sizes_of_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afdb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def get_trunctad_logits_tiling(protein_seq, seq_name, model, alphabet, rep_layer):\n",
    "    # \"\"\"Get logits for long sequences using tiling approach.\"\"\"\n",
    "# \n",
    "    # return process_long_sequence_chunking_with_overlapping_regions(alphabet, seq_name, protein_seq, model, rep_layer)\n",
    "\n",
    "\n",
    "def process_long_sequence_chunking_with_overlapping_regions(alphabet, seq_name, protein_seq, model,\n",
    "                                                            rep_layer):\n",
    "    \"\"\"\n",
    "    Process long protein sequences by chunking with overlapping regions.\n",
    "    Ensures that <mask> tokens are never split across chunks.\n",
    "\n",
    "    Args:\n",
    "        alphabet: The ESM alphabet\n",
    "        seq_name: Name of the sequence\n",
    "        protein_seq: The full protein sequence\n",
    "        model: The ESM model\n",
    "        esm_model_name: Name of the ESM model\n",
    "        rep_layer: Representation layer to use\n",
    "\n",
    "    Returns:\n",
    "        Combined logits for the entire sequence\n",
    "    \"\"\"\n",
    "\n",
    "    seq_length = len(protein_seq)\n",
    "    chunk_size = d.ESM_MAX_LENGTH\n",
    "\n",
    "    # Find all mask positions in the sequence \n",
    "    mask_positions = []\n",
    "    start_pos = 0\n",
    "    while True:\n",
    "        mask_pos = protein_seq.find(d.MASK_TOKEN, start_pos)\n",
    "        if mask_pos == -1:\n",
    "            break\n",
    "        mask_positions.append((mask_pos, mask_pos + len(d.MASK_TOKEN)))\n",
    "        start_pos = mask_pos + len(d.MASK_TOKEN)\n",
    "\n",
    "\n",
    "    def would_split_mask(chunk_start, chunk_end, mask_positions):\n",
    "        \"\"\"Check if the chunk boundaries would split any mask token.\"\"\"\n",
    "        for mask_start, mask_end in mask_positions:\n",
    "            # Check if mask is partially inside the chunk (i.e., would be split)\n",
    "            if (chunk_start < mask_end and chunk_end > mask_start and\n",
    "                    not (chunk_start <= mask_start and chunk_end >= mask_end)):\n",
    "                return True, mask_start, mask_end\n",
    "        return False, None, None\n",
    "\n",
    "    chunk_positions = []\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "\n",
    "    while start_pos < seq_length and end_pos < seq_length:\n",
    "        end_pos = min(start_pos + chunk_size, seq_length)\n",
    "\n",
    "        # Check if this chunk would split a mask\n",
    "        would_split, mask_start, mask_end = would_split_mask(start_pos, end_pos, mask_positions)\n",
    "\n",
    "        if would_split:\n",
    "            # Adjust chunk_end to not split the mask\n",
    "            if mask_start >= start_pos:\n",
    "                # Mask starts within or after chunk start - end chunk before mask\n",
    "                end_pos = mask_start\n",
    "            else:\n",
    "                # Mask starts before chunk start but ends within chunk - this shouldn't happen with proper chunking\n",
    "                # but handle it by including the entire mask\n",
    "                end_pos = mask_end\n",
    "\n",
    "        # If the adjusted chunk is too small (less than overlap size),\n",
    "        # we need to include the mask in this chunk\n",
    "        if end_pos - start_pos < d.OVERLAP_SIZE_LONG_PROTEIN and would_split:\n",
    "            # Find the mask that's causing the issue and include it\n",
    "            for mask_start, mask_end in mask_positions:\n",
    "                if mask_start < start_pos + chunk_size and mask_end > end_pos:\n",
    "                    end_pos = min(mask_end, seq_length)\n",
    "                    break\n",
    "\n",
    "        # Skip empty chunks\n",
    "        if end_pos <= start_pos:\n",
    "            start_pos += 1\n",
    "            continue\n",
    "\n",
    "        # Define the valid region (excluding padding)\n",
    "        valid_start = d.OVERLAP_SIZE_LONG_PROTEIN if start_pos > 0 else 0\n",
    "        valid_end = (end_pos - start_pos) - d.OVERLAP_SIZE_LONG_PROTEIN if end_pos < seq_length else (end_pos - start_pos)\n",
    "\n",
    "        # Ensure valid_end doesn't exceed chunk size\n",
    "        valid_end = min(valid_end, end_pos - start_pos)\n",
    "\n",
    "        # Ensure we have a valid region\n",
    "        if valid_end <= valid_start:\n",
    "            valid_end = end_pos - start_pos\n",
    "\n",
    "        chunk_positions.append({\n",
    "            'chunk_start': start_pos,\n",
    "            'chunk_end': end_pos,\n",
    "            'valid_start': valid_start,\n",
    "            'valid_end': valid_end\n",
    "        })\n",
    "\n",
    "\n",
    "        # Slide the window, considering the overlap\n",
    "        if end_pos >= seq_length:\n",
    "            break\n",
    "\n",
    "        # Calculate next start position\n",
    "        next_start = start_pos + chunk_size - 2 * d.OVERLAP_SIZE_LONG_PROTEIN\n",
    "\n",
    "        # Make sure we don't start in the middle of a mask token\n",
    "        for mask_start, mask_end in mask_positions:\n",
    "            if mask_start < next_start < mask_end:\n",
    "                # Adjust start to after the mask\n",
    "                next_start = mask_end\n",
    "                break\n",
    "\n",
    "        start_pos = next_start\n",
    "\n",
    "    # Process each chunk\n",
    "    final_logits = None\n",
    "\n",
    "    for i, pos in enumerate(chunk_positions):\n",
    "        # Extract the chunk\n",
    "        chunk_seq = protein_seq[pos['chunk_start']:pos['chunk_end']]\n",
    "\n",
    "        # Process the chunk\n",
    "        batch_tokens = get_batch_token(alphabet, seq_name, chunk_seq)\n",
    "        chunk_logits = get_trunctad_logits(True, batch_tokens, model, rep_layer)\n",
    "        chunk_logits = chunk_logits.squeeze(0)\n",
    "\n",
    "        # Extract the valid region\n",
    "        valid_logits = chunk_logits[pos['valid_start']:pos['valid_end']]\n",
    "\n",
    "        # Append to the combined logits\n",
    "        if final_logits is None:\n",
    "            final_logits = valid_logits\n",
    "        else:\n",
    "            final_logits = torch.cat([final_logits, valid_logits], dim=0)\n",
    "\n",
    "    return final_logits\n",
    "\n",
    "\n",
    "def get_batch_token(alphabet, example_name, sequence):\n",
    "    tokenizer = alphabet.get_batch_converter()\n",
    "    input = [(example_name, sequence)]\n",
    "    _, _, batch_tokens = tokenizer(input)\n",
    "    batch_tokens = batch_tokens.to(d.DEVICE)\n",
    "    return batch_tokens\n",
    "\n",
    "\n",
    "def get_trunctad_logits(aa_only, batch_tokens, model, rep_layers):\n",
    "    chunk_logits = model(batch_tokens, repr_layers=rep_layers, return_contacts=False)['logits']\n",
    "    logit_parts = []\n",
    "    # logit_parts.append(chunk_logits[0, 1:-1, 4:24] if aa_only else chunk_logits[0, 1:-1, :])\n",
    "    logit_parts.append(chunk_logits[0, 1:, 4:24] if aa_only else chunk_logits[0, 1:-1, :])\n",
    "    return torch.stack(logit_parts).to(d.DEVICE)\n",
    "\n",
    "\n",
    "def get_mutant_dest_and_seq(method_mutant, sequence, aa_mut):\n",
    "    if method_mutant == METHODS_TO_ESM.MUTANTE:\n",
    "        mutant_seq = sequence[:aa_mut.mut_idx] + aa_mut.change_aa + sequence[aa_mut.mut_idx + 1:]\n",
    "    elif method_mutant == METHODS_TO_ESM.MASKED:\n",
    "        mutant_seq = sequence[:aa_mut.mut_idx] + \"<mask>\" + sequence[aa_mut.mut_idx + 1:]\n",
    "    elif method_mutant == METHODS_TO_ESM.WT:\n",
    "        mutant_seq = sequence\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method_mutant: {method_mutant}\")\n",
    "    return mutant_seq\n",
    "\n",
    "\n",
    "def run_esm_without_poem(model, alphabet, rep_layers, protein_seq, mutation_desc):\n",
    "    mutation_variant_set = MutationVariantSet()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        aa_mut = utils.process_mutation_name(mutation_desc)\n",
    "        for method_mutant in METHODS_TO_ESM.get_methods():\n",
    "            mutant_seq = get_mutant_dest_and_seq(method_mutant, protein_seq, aa_mut)\n",
    "            seq_name = f\"{method_mutant.value}_{mutation_desc}\"\n",
    "            truncated_logits = process_long_sequence_chunking_with_overlapping_regions(alphabet, seq_name, mutant_seq, model, rep_layers)\n",
    "\n",
    "            mutant_record = MutationRecord(\n",
    "                protein_seq=protein_seq,\n",
    "                aa_mut=aa_mut,\n",
    "                truncated_logits=truncated_logits,\n",
    "            )\n",
    "            mutation_variant_set.add_mutation_record(mutant_record, method_mutant)\n",
    "\n",
    "    return mutation_variant_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "        \n",
    "def classify_mutation_to_tree_path(homolog_count, is_disordered, sequence_length):\n",
    "    \"\"\"\n",
    "    Helper function to classify a mutation to its tree path based on its characteristics.\n",
    "    \n",
    "    Args:\n",
    "        homolog_count: Number of same sequence in cluster\n",
    "        is_disordered: Boolean indicating if mutation is in disordered region\n",
    "        sequence_length: Length of the protein sequence\n",
    "        \n",
    "    Returns:\n",
    "        Tree path string that can be used as a key\n",
    "    \"\"\"\n",
    "    # Homolog classification\n",
    "    if homolog_count <= 450:\n",
    "        homolog_part = \"homologs_0_to_450\"\n",
    "    else:\n",
    "        homolog_part = \"homologs_450_plus\"\n",
    "    \n",
    "    # Disorder classification\n",
    "    if is_disordered:\n",
    "        disorder_part = \"disordered\"\n",
    "    else:\n",
    "        disorder_part = \"ordered\"\n",
    "    \n",
    "    # Length classification\n",
    "    if sequence_length <= 1022:\n",
    "        length_part = \"shorter_than_1022\"\n",
    "    else:\n",
    "        length_part = \"longer_than_1022\"\n",
    "    \n",
    "    return f\"all/{homolog_part}/{disorder_part}/{length_part}\"\n",
    "\n",
    "\n",
    "def get_pathogenic_percentage(tree_path_key: str, leaf_best_scores: dict, bins_dict: dict) -> tuple[str, float]:\n",
    "    \"\"\"Get the pathogenic percentage for a mutation based on its tree path key.\n",
    "\n",
    "    Args:\n",
    "        tree_path_key (str): The key representing the tree path for the mutation.\n",
    "        leaf_best_scores (dict): A dictionary containing the best scores for each tree path.\n",
    "        bins_dict (dict): A dictionary containing bin information for each tree path.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, float]: A tuple containing the bin index as a string and the pathogenic percentage.\n",
    "    \"\"\"\n",
    "    bin_key, score_llr = leaf_best_scores[tree_path_key]\n",
    "\n",
    "    # Get the bin information\n",
    "    bin_info = bins_dict[tree_path_key][bin_key]\n",
    "    bin_edges = np.array(bin_info['bin_edges'])\n",
    "    bin_stats = bin_info['bin_stats']\n",
    "\n",
    "    # Find which bin the score falls into\n",
    "    bin_index = np.digitize(score_llr, bin_edges) - 1\n",
    "\n",
    "    # Handle edge cases\n",
    "    if bin_index < 0:\n",
    "        bin_index = 0\n",
    "    elif bin_index >= len(bin_edges) - 1:\n",
    "        bin_index = len(bin_edges) - 2\n",
    "\n",
    "    # Convert bin_index to string (as that's how it's stored in bin_stats)\n",
    "    bin_index_str = str(bin_index)\n",
    "    pathogenic_percentage = bin_stats[int(bin_index_str)]['path_pct']\n",
    "\n",
    "    return score_llr, pathogenic_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8510f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_parquet(r\"c:\\Users\\sapir\\out_of_onedrive\\dina\\entropy-missense-prediction\\resources\\2_sort_runnings\\preprocessed_P53_HUMAN.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9447ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHERCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELPPGSTKRALPNNTSSSPQPKKKPLDGEYFTLQIRGRERFEMFRELNEALELKDAQAGKEPGGSRAHSSHLKSKKGQSTSRHKKLMFKTEGPDSD\"\n",
    "mutation = \"M1A\"\n",
    "# esm_model_name = \"esm1b_t33_650M_UR50S\"\n",
    "esm_model_name = \"esm1_t6_43M_UR50S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fb8a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sapir/.cache\\torch\\hub\\facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "orig_model, orig_alphabet = utils.esm_setup(esm_model_name, device=d.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77f752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_variant_set = run_esm_without_poem(orig_model, orig_alphabet, [33], seq, mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eaeaebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_dict_path = \"data/bins_dict.pkl\"\n",
    "\n",
    "with open(bins_dict_path, \"rb\") as f:\n",
    "    bins_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "leaf_best_scores = {\n",
    "    'all/homologs_0_to_450/disordered/longer_than_1022': ['wt_marginals_base_wt_score_126', mutation_variant_set.wt.nadav_base_wt_score.item()],\n",
    "    'all/homologs_0_to_450/disordered/shorter_than_1022': ['masked_marginals_entropy_weighted_llr_score_142', mutation_variant_set.masked.entropy_weighted_llr_score.item()],\n",
    "    'all/homologs_0_to_450/ordered/longer_than_1022': ['wt_marginals_base_wt_score_200', mutation_variant_set.wt.nadav_base_wt_score.item()],\n",
    "    'all/homologs_0_to_450/ordered/shorter_than_1022': ['mutant_marginals_entropy_weighted_llr_score_200', mutation_variant_set.mutante.entropy_weighted_llr_score.item()],\n",
    "    'all/homologs_450_plus/disordered/longer_than_1022': ['wt_not_nadav_marginals_base_wt_score_203', mutation_variant_set.wt.llr_base_score.item()],\n",
    "    'all/homologs_450_plus/disordered/shorter_than_1022': ['wt_not_nadav_marginals_base_wt_score_77', mutation_variant_set.wt.llr_base_score.item()],\n",
    "    'all/homologs_450_plus/ordered/longer_than_1022': ['mutant_marginals_entropy_weighted_llr_score_200', mutation_variant_set.mutante.entropy_weighted_llr_score.item()],\n",
    "    'all/homologs_450_plus/ordered/shorter_than_1022': ['masked_marginals_entropy_weighted_llr_score_200', mutation_variant_set.masked.entropy_weighted_llr_score.item()],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c27e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/new_clusters_sequences.parquet'\n",
    "homolog_count = get_cluster_sizes_of_sequence(seq, 'data', 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aa_mut = utils.process_mutation_name(mutation)\n",
    "is_disorder_region = is_disordered(seq, aa_mut.mut_idx)\n",
    "tree_path_key = classify_mutation_to_tree_path(\n",
    "    homolog_count=data_path,\n",
    "    is_disordered=is_disorder_region,\n",
    "    sequence_length=len(seq)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f897903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Path Key: all/homologs_0_to_450/disordered/shorter_than_1022\n",
      "Bin Index: 3, Pathogenic Percentage: 52.82%\n"
     ]
    }
   ],
   "source": [
    "score_llr, pathogenic_percentage = get_pathogenic_percentage(tree_path_key, leaf_best_scores, bins_dict)\n",
    "print(f\"Tree Path Key: {tree_path_key}\")\n",
    "print(f\"Score LLR: {score_llr}, Pathogenic Percentage: {pathogenic_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131186b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_image_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
